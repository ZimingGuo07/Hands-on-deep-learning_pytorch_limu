{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.标量与变量\n",
      "tensor([5.]) tensor([6.]) tensor([1.5000]) tensor([9.])\n",
      "2.向量\n",
      "x: tensor([0, 1, 2, 3])\n",
      "x[3]: tensor(3)\n",
      "张量的形状: torch.Size([4])\n",
      "张量的长度: 4\n",
      "三维张量的长度: 2\n",
      "3.矩阵\n",
      "A: tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [12, 13, 14, 15],\n",
      "        [16, 17, 18, 19]])\n",
      "A.shape: torch.Size([5, 4])\n",
      "A.shape[0]: 5\n",
      "A.T: tensor([[ 0,  4,  8, 12, 16],\n",
      "        [ 1,  5,  9, 13, 17],\n",
      "        [ 2,  6, 10, 14, 18],\n",
      "        [ 3,  7, 11, 15, 19]])\n",
      "4.矩阵的计算\n",
      "A: tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]])\n",
      "B: tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]])\n",
      "A + B: tensor([[ 0.,  2.,  4.,  6.],\n",
      "        [ 8., 10., 12., 14.],\n",
      "        [16., 18., 20., 22.],\n",
      "        [24., 26., 28., 30.],\n",
      "        [32., 34., 36., 38.]])\n",
      "A * B: tensor([[  0.,   1.,   4.,   9.],\n",
      "        [ 16.,  25.,  36.,  49.],\n",
      "        [ 64.,  81., 100., 121.],\n",
      "        [144., 169., 196., 225.],\n",
      "        [256., 289., 324., 361.]])\n",
      "X: tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7],\n",
      "         [ 8,  9, 10, 11]],\n",
      "\n",
      "        [[12, 13, 14, 15],\n",
      "         [16, 17, 18, 19],\n",
      "         [20, 21, 22, 23]]])\n",
      "a + X: tensor([[[ 2,  3,  4,  5],\n",
      "         [ 6,  7,  8,  9],\n",
      "         [10, 11, 12, 13]],\n",
      "\n",
      "        [[14, 15, 16, 17],\n",
      "         [18, 19, 20, 21],\n",
      "         [22, 23, 24, 25]]])\n",
      "a * X: tensor([[[ 0,  2,  4,  6],\n",
      "         [ 8, 10, 12, 14],\n",
      "         [16, 18, 20, 22]],\n",
      "\n",
      "        [[24, 26, 28, 30],\n",
      "         [32, 34, 36, 38],\n",
      "         [40, 42, 44, 46]]])\n",
      "torch.Size([2, 3, 4])\n",
      "5.矩阵的sum运算\n",
      "A: tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]])\n",
      "A.shape: torch.Size([5, 4])\n",
      "A.sum(): tensor(190.)\n",
      "A.sum(axis=0): tensor([40., 45., 50., 55.])\n",
      "A.sum(axis=1): tensor([ 6., 22., 38., 54., 70.])\n",
      "A.sum(axis=1, keepdims=True) tensor([[ 6.],\n",
      "        [22.],\n",
      "        [38.],\n",
      "        [54.],\n",
      "        [70.]])\n",
      "A.sum(axis=[0, 1]): tensor(190.)\n",
      "A.mean(): tensor(9.5000)\n",
      "A.sum() / A.numel(): tensor(9.5000)\n",
      "A.mean(axis=0): tensor([ 8.,  9., 10., 11.])\n",
      "A.mean(axis=1): tensor([ 1.5000,  5.5000,  9.5000, 13.5000, 17.5000])\n",
      "6.向量-向量相乘（点积）\n",
      "x: tensor([0., 1., 2., 3.])\n",
      "y: tensor([1., 1., 1., 1.])\n",
      "向量-向量点积: tensor(6.)\n",
      "7.矩阵-向量相乘(向量积)\n",
      "A: tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]])\n",
      "x: tensor([0., 1., 2., 3.])\n",
      "torch.mv(A, x): tensor([ 14.,  38.,  62.,  86., 110.])\n",
      "8.矩阵-矩阵相乘(向量积)\n",
      "A: tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]])\n",
      "B: tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "torch.mm(A, B): tensor([[ 6.,  6.,  6.],\n",
      "        [22., 22., 22.],\n",
      "        [38., 38., 38.],\n",
      "        [54., 54., 54.],\n",
      "        [70., 70., 70.]])\n",
      "9.范数\n",
      "向量的𝐿2范数: tensor(5.)\n",
      "向量的𝐿1范数: tensor(7.)\n",
      "v: tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "矩阵的𝐿2范数: tensor(6.)\n",
      "矩阵的弗罗贝尼乌斯范数（F范数）： tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print('1.标量与变量')\n",
    "x = torch.tensor([3.0])\n",
    "y = torch.tensor([2.0])\n",
    "print(x + y, x * y, x / y, x ** y)\n",
    "\n",
    "\n",
    "print('2.向量')\n",
    "x = torch.arange(4)\n",
    "print('x:', x)\n",
    "print('x[3]:', x[3])  # 通过张量的索引来访问任一元素\n",
    "print('张量的形状:', x.shape)  # 张量的形状\n",
    "print('张量的长度:', len(x))  # 张量的长度\n",
    "z = torch.arange(24).reshape(2, 3, 4)\n",
    "print('三维张量的长度:', len(z))\n",
    "\n",
    "\n",
    "print('3.矩阵')\n",
    "A = torch.arange(20).reshape(5, 4)\n",
    "print('A:', A)\n",
    "print('A.shape:', A.shape)\n",
    "print('A.shape[0]:', A.shape[0])\n",
    "print('A.T:', A.T)  # 矩阵的转置\n",
    "\n",
    "\n",
    "print('4.矩阵的计算')\n",
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "B = A.clone()  # 通过分配新内存，将A的一个副本分配给B\n",
    "print('A:', A)\n",
    "print('B:', B)\n",
    "print('A + B:', A + B)  # 矩阵相加\n",
    "print('A * B:', A * B)  # 矩阵相乘\n",
    "\n",
    "a = 2\n",
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "print('X:', X)\n",
    "print('a + X:', a + X)  # 矩阵的值加上标量\n",
    "print('a * X:', a * X)\n",
    "print((a * X).shape)\n",
    "\n",
    "\n",
    "print('5.矩阵的sum运算')\n",
    "#对于二维数据，0表示纵轴，方向从上到下，1表示横轴，方向从左到右。注意方向\n",
    "print('A:', A)\n",
    "print('A.shape:', A.shape)\n",
    "print('A.sum():', A.sum())\n",
    "print('A.sum(axis=0):', A.sum(axis=0))  # 沿0轴汇总以生成输出向量\n",
    "print('A.sum(axis=1):', A.sum(axis=1))  # 沿1轴汇总以生成输出向量\n",
    "print('A.sum(axis=1, keepdims=True)', A.sum(axis=1, keepdims=True))  # 计算总和保持轴数不变\n",
    "print('A.sum(axis=[0, 1]):', A.sum(axis=[0, 1]))  # Same as `A.sum()`\n",
    "print('A.mean():', A.mean())\n",
    "print('A.sum() / A.numel():', A.sum() / A.numel())\n",
    "print('A.mean(axis=0):', A.mean(axis=0))  # 沿0轴汇总以生成输出向量\n",
    "print('A.mean(axis=1):', A.mean(axis=1))  # 沿1轴汇总以生成输出向量\n",
    "\n",
    "\n",
    "print('6.向量-向量相乘（点积）')\n",
    "x = torch.arange(4, dtype=torch.float32)\n",
    "y = torch.ones(4, dtype=torch.float32)\n",
    "print('x:', x)\n",
    "print('y:', y)\n",
    "print('向量-向量点积:', torch.dot(x, y))\n",
    "\n",
    "\n",
    "print('7.矩阵-向量相乘(向量积)')\n",
    "print('A:', A)  # 5*4维\n",
    "print('x:', x)  # 4*1维\n",
    "print('torch.mv(A, x):', torch.mv(A, x))\n",
    "\n",
    "\n",
    "print('8.矩阵-矩阵相乘(向量积)')\n",
    "print('A:', A)  # 5*4维\n",
    "B = torch.ones(4, 3)  # 4*3维\n",
    "print('B:', B)\n",
    "print('torch.mm(A, B):', torch.mm(A, B))\n",
    "\n",
    "\n",
    "print('9.范数')\n",
    "u = torch.tensor([3.0, -4.0])\n",
    "print('向量的𝐿2范数:', torch.norm(u))  # 向量的𝐿2范数   向量元素平方和的平方根\n",
    "print('向量的𝐿1范数:', torch.abs(u).sum())  # 向量的𝐿1范数   向量元素的绝对值之和\n",
    "v = torch.ones((4, 9))\n",
    "print('v:', v)\n",
    "print('矩阵的𝐿2范数:', torch.norm(v))  # 矩阵的𝐿2范数\n",
    "print(\"矩阵的弗罗贝尼乌斯范数（F范数）：\",torch.norm(torch.ones((4,9))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5179d32cf6ec497baf3f8a3ef987cc77c5d2dc691fdde20a56316522f61a7323"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
