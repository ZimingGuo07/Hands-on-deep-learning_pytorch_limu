{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于数据读取的问题\n",
    "一般来说PyTorch中深度学习训练的流程是这样的： 1. 创建Dateset 2. Dataset传递给DataLoader 3. DataLoader迭代产生训练数据提供给模型\n",
    "代码如下\n",
    "```python\n",
    "# 创建Dateset(可以自定义)\n",
    "    dataset = face_dataset # Dataset部分自定义过的face_dataset\n",
    "# Dataset传递给DataLoader\n",
    "    dataloader = torch.utils.data.DataLoader(dataset,batch_size=64,shuffle=False,num_workers=8)\n",
    "# DataLoader迭代产生训练数据提供给模型\n",
    "    for i in range(epoch):\n",
    "        for index,(img,label) in enumerate(dataloader):\n",
    "            pass\n",
    "```\n",
    "其中 `for index,(img,label) in enumerate(dataloader):` 也可以使用 iter() 方法替换，比如 `for img,label in iter(dataloader):` \n",
    " dataloader 本质上是一个可迭代对象，可以使用 iter() 进行访问，采用 iter(dataloader) 返回的是一个迭代器，然后可以使用 next() 访问。\n",
    "也可以使用 enumerate(dataloader) 的形式访问。\n",
    "[Pytorch中iter(dataloader)的使用](https://blog.csdn.net/weixin_44533869/article/details/110856518)这篇文章中介绍了这两种方法及其使用，但是在使用 enumerate(dataloader) 时没有加 index 导致 img 和 label 的位置对换了，实测加上 index 后无误，即示例代码的样子。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1:\n",
      "Net1(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (dense1): Linear(in_features=288, out_features=128, bias=True)\n",
      "  (dense2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n",
      "Method 2:\n",
      "Net2(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (dense): Sequential(\n",
      "    (0): Linear(in_features=288, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Method 2.0:\n",
      "Sequential(\n",
      "  (0): Linear(in_features=20, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#搭建神经网络的几种方法   1、2是感觉比较好用的两种方法,2.0是第2种方法当只有一个模块的情况下的简化形式，也比较方便。\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "\n",
    "#1、继承Module类\n",
    "class Net1(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net1, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(3, 32, 3, 1, 1)\n",
    "        self.dense1 = torch.nn.Linear(32 * 3 * 3, 128)\n",
    "        self.dense2 = torch.nn.Linear(128, 10)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv(x)), 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.dense1(x))\n",
    "        x = self.dense2(x)\n",
    "        return x\n",
    " \n",
    "print(\"Method 1:\")\n",
    "model1 = Net1()\n",
    "print(model1)\n",
    "print('这种方法比较常用，早期的教程通常就是使用这种方法。')\n",
    "\n",
    "#2、Sequential类\n",
    "class Net2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "        self.conv = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, 32, 3, 1, 1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2))\n",
    "        self.dense = torch.nn.Sequential(\n",
    "            torch.nn.Linear(32 * 3 * 3, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 10)\n",
    "        )\n",
    " \n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv1(x)\n",
    "        res = conv_out.view(conv_out.size(0), -1)\n",
    "        out = self.dense(res)\n",
    "        return out\n",
    " \n",
    "print(\"Method 2:\")\n",
    "model2 = Net2()\n",
    "print(model2)\n",
    "print('这种方法利用torch.nn.Sequential()容器进行快速搭建，模型的各层被顺序添加到容器中。缺点是每层的编号是默认的阿拉伯数字，不易区分。')\n",
    "\n",
    "#2.0 2的简化情况\n",
    "print(\"Method 2.0:\")\n",
    "net0 = torch.nn.Sequential(torch.nn.Linear(20, 256), torch.nn.ReLU(), torch.nn.Linear(256, 10))\n",
    "print(net0)\n",
    "\n",
    "#3、2的改进（自己用的话感觉用不到这个）\n",
    "class Net3(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net3, self).__init__()\n",
    "        self.conv = torch.nn.Sequential()\n",
    "        self.conv.add_module(\"conv1\",torch.nn.Conv2d(3, 32, 3, 1, 1))\n",
    "        self.conv.add_module(\"relu1\",torch.nn.ReLU())\n",
    "        self.conv.add_module(\"pool1\",torch.nn.MaxPool2d(2))\n",
    "        self.dense = torch.nn.Sequential()\n",
    "        self.dense.add_module(\"dense1\",torch.nn.Linear(32 * 3 * 3, 128))\n",
    "        self.dense.add_module(\"relu2\",torch.nn.ReLU())\n",
    "        self.dense.add_module(\"dense2\",torch.nn.Linear(128, 10))\n",
    " \n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv1(x)\n",
    "        res = conv_out.view(conv_out.size(0), -1)\n",
    "        out = self.dense(res)\n",
    "        return out\n",
    " \n",
    "print(\"Method 3:\")\n",
    "model3 = Net3()\n",
    "print(model3)\n",
    "print(\"这种方法是对第二种方法的改进：通过add_module()添加每一层，并且为每一层增加了一个单独的名字。\")\n",
    "\n",
    "#4、3的改写\n",
    "class Net4(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net4, self).__init__()\n",
    "        self.conv = torch.nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (\"conv1\", torch.nn.Conv2d(3, 32, 3, 1, 1)),\n",
    "                    (\"relu1\", torch.nn.ReLU()),\n",
    "                    (\"pool\", torch.nn.MaxPool2d(2))\n",
    "                ]\n",
    "            ))\n",
    " \n",
    "        self.dense = torch.nn.Sequential(\n",
    "            OrderedDict([\n",
    "                (\"dense1\", torch.nn.Linear(32 * 3 * 3, 128)),\n",
    "                (\"relu2\", torch.nn.ReLU()),\n",
    "                (\"dense2\", torch.nn.Linear(128, 10))\n",
    "            ])\n",
    "        )\n",
    " \n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv1(x)\n",
    "        res = conv_out.view(conv_out.size(0), -1)\n",
    "        out = self.dense(res)\n",
    "        return out\n",
    " \n",
    "print(\"Method 4:\")\n",
    "model4 = Net4()\n",
    "print(model4)\n",
    "print(\"第三种方法的另外一种写法，通过字典的形式添加每一层，并且设置单独的层名称。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于训练过程，基本的格式就是 嵌套循环 + 遍历一次数据 + 打印loss\n",
    "\n",
    "1、嵌套循环\n",
    "```\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter:#这里还可以使用enumerate方法来访问dataloader\n",
    "```\n",
    "\n",
    "2、遍历一次数据\n",
    "主要分为3个部分\n",
    "（1）、通过调用net(X)生成预测并计算损失loss（前向传播）。\n",
    "```\n",
    "loss = loss_func(net(x), y)\n",
    "```\n",
    "（2）、通过进行反向传播来计算梯度。\n",
    "```\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "```\n",
    "（3）、通过调用优化器来更新模型参数。\n",
    "```\n",
    "optimizer.step()\n",
    "```\n",
    "\n",
    "3、打印loss\n",
    "![打印loss](picture\\打印loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_shape: torch.Size([1000])\n",
      "X: tensor([[-1.6564, -0.7528],\n",
      "        [-0.4417, -0.4678],\n",
      "        [-0.5204, -0.2620],\n",
      "        [-0.9958,  2.4005],\n",
      "        [-0.1910, -0.6708],\n",
      "        [ 1.4716, -2.0913],\n",
      "        [ 0.1304,  1.8677],\n",
      "        [ 0.7979, -1.5860],\n",
      "        [ 0.4219, -0.5894],\n",
      "        [-0.3467,  1.3199]]) \n",
      "y tensor([[ 3.4553],\n",
      "        [ 4.9128],\n",
      "        [ 4.0512],\n",
      "        [-5.9535],\n",
      "        [ 6.1058],\n",
      "        [14.2364],\n",
      "        [-1.8966],\n",
      "        [11.2016],\n",
      "        [ 7.0429],\n",
      "        [-0.9917]])\n",
      "w tensor([[ 1.9383],\n",
      "        [-3.2635]], requires_grad=True) \n",
      "b tensor([4.0289], requires_grad=True) \n",
      "loss 0.027315\n",
      "w tensor([[ 1.9986],\n",
      "        [-3.3943]], requires_grad=True) \n",
      "b tensor([4.1929], requires_grad=True) \n",
      "loss 0.000096\n",
      "w tensor([[ 2.0000],\n",
      "        [-3.3995]], requires_grad=True) \n",
      "b tensor([4.1995], requires_grad=True) \n",
      "loss 0.000052\n",
      "w误差  tensor([[-2.8849e-05, -5.4000e+00],\n",
      "        [ 5.3995e+00, -5.3883e-04]], grad_fn=<SubBackward0>) \n",
      "b误差  tensor([0.0005], grad_fn=<RsubBackward1>)\n"
     ]
    }
   ],
   "source": [
    "#简洁实现\n",
    "\n",
    "import random\n",
    "\n",
    "import torch\n",
    "\n",
    "## Python3 range() 函数返回的是一个可迭代对象（类型是对象），而不是列表类型， 所以打印的时候不会打印列表。\n",
    "## Python3 list() 函数是对象迭代器，可以把range()返回的可迭代对象转为一个列表，返回的变量类型为列表。\n",
    "\n",
    "\n",
    "## 人造数据集\n",
    "def create_data(w, b, nums_example):\n",
    "    X = torch.normal(0, 1, (nums_example, len(w)))\n",
    "    y = torch.matmul(X, w) + b\n",
    "    print(\"y_shape:\", y.shape)\n",
    "    y += torch.normal(0, 0.01, y.shape)  # 加入噪声\n",
    "    return X, y.reshape(-1, 1)  # y从行向量转为列向量\n",
    "\n",
    "\n",
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "features, labels = create_data(true_w, true_b, 1000)\n",
    "\n",
    "\n",
    "## 读数据集\n",
    "def read_data(batch_size, features, lables):\n",
    "    nums_example = len(features)\n",
    "    indices = list(range(nums_example))  # 生成0-999的元组，然后将range()返回的可迭代对象转为一个列表\n",
    "    random.shuffle(indices)  # 将序列的所有元素随机排序。\n",
    "    for i in range(0, nums_example, batch_size):  # range(start, stop, step)\n",
    "        index_tensor = torch.tensor(indices[i: min(i + batch_size, nums_example)])\n",
    "        yield features[index_tensor], lables[index_tensor]  # 通过索引访问向量\n",
    "\n",
    "\n",
    "batch_size = 10\n",
    "for X, y in read_data(batch_size, features, labels):\n",
    "    print(\"X:\", X, \"\\ny\", y)\n",
    "    break;\n",
    "\n",
    "##初始化参数\n",
    "w = torch.normal(0, 0.01, size=(2, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "\n",
    "# 定义模型\n",
    "def net(X, w, b):\n",
    "    return torch.matmul(X, w) + b\n",
    "\n",
    "\n",
    "# 定义损失函数\n",
    "def loss(y_hat, y):\n",
    "    # print(\"y_hat_shape:\",y_hat.shape,\"\\ny_shape:\",y.shape)\n",
    "    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2  # 这里为什么要加 y_hat_shape: torch.Size([10, 1])  y_shape: torch.Size([10])\n",
    "\n",
    "\n",
    "# 定义优化算法\n",
    "def sgd(params, batch_size, lr):\n",
    "    with torch.no_grad():  # with torch.no_grad() 则主要是用于停止autograd模块的工作，\n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size  ##  这里用param = param - lr * param.grad / batch_size会导致导数丢失， zero_()函数报错\n",
    "            param.grad.zero_()  ## 导数如果丢失了，会报错‘NoneType’ object has no attribute ‘zero_’\n",
    "\n",
    "\n",
    "# 训练模型\n",
    "lr = 0.03\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(0, num_epochs):\n",
    "    for X, y in read_data(batch_size, features, labels):\n",
    "        f = loss(net(X, w, b), y)\n",
    "        # 因为`f`形状是(`batch_size`, 1)，而不是一个标量。`f`中的所有元素被加到一起，\n",
    "        # 并以此计算关于[`w`, `b`]的梯度\n",
    "        f.sum().backward()\n",
    "        sgd([w, b], batch_size, lr)  # 使用参数的梯度更新参数\n",
    "    with torch.no_grad():\n",
    "        train_l = loss(net(features, w, b), labels)\n",
    "        print(\"w {0} \\nb {1} \\nloss {2:f}\".format(w, b, float(train_l.mean())))\n",
    "\n",
    "print(\"w误差 \", true_w - w, \"\\nb误差 \", true_b - b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5179d32cf6ec497baf3f8a3ef987cc77c5d2dc691fdde20a56316522f61a7323"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
