{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基础优化算法指的是SGD，其后续发展在语雀里讲解过，这里的以后会继续写，这里先放篇文章[从 SGD 到 Adam —— 深度学习优化算法概览(一)](https://zhuanlan.zhihu.com/p/32626442)    ||   [Adam 究竟还有什么问题 —— 深度学习优化算法概览(二)](https://zhuanlan.zhihu.com/p/37269222)\n",
    "\n",
    "w = w - lr*▽f(w)\n",
    "lr为学习率，▽f(w)为w点时的梯度（函数变化最快的方向），证明见[梯度是不是最快的下降方向？](https://www.zhihu.com/question/461309224/answer/1908400480)    ||   [梯度的方向为什么是函数值增加最快的方向？](https://zhuanlan.zhihu.com/p/38525412)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、数据预处理的归一化对于能否收敛到全局最优解也很重要，假设在一个只有两个参数的例子中，横纵坐标的参数尺度不同会导致出现一个椭圆而不是圆，显然圆比椭圆更容易收敛。SGD还会涉及到学习率的大小问题，太小会导致学习速度慢，太大会导致震荡从而无法收敛到全局最优解。\n",
    "\n",
    "2、还有一个问题是鞍点和局部最小值的问题，由于SGD的变化方向仅取决于梯度，这会导致在局部最小值处不动和在鞍点处前进速度极为缓慢。\n",
    "\n",
    "3、SGD会使用一个batchsize的平均梯度来近似反映数据整体的梯度方向，batchsize太小虽然可以反映出每个数据的变化情况但是不适合并行计算，浪费资源，batchsize太大则导致每次计算结果很可能相同，大量数据同时计算浪费内存\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78fc45657d17e5ae40105ec575be94d3851fcfd0a2ad7fcbb61c064bd85d2aa1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
